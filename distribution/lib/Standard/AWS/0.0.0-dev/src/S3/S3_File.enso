from Standard.Base import all
import Standard.Base.Enso_Cloud.Data_Link
import Standard.Base.Errors.Common.Syntax_Error
import Standard.Base.Errors.File_Error.File_Error
import Standard.Base.Errors.Illegal_Argument.Illegal_Argument
import Standard.Base.Errors.Unimplemented.Unimplemented
import Standard.Base.Runtime.Context
import Standard.Base.System.File.Generic.File_Like.File_Like
import Standard.Base.System.File.Generic.Writable_File.Writable_File
import Standard.Base.System.File_Format_Metadata.File_Format_Metadata
import Standard.Base.System.Input_Stream.Input_Stream
import Standard.Base.System.Output_Stream.Output_Stream
from Standard.Base.System.File import find_extension_from_name
from Standard.Base.System.File.Generic.File_Write_Strategy import generic_copy

import project.AWS_Credential.AWS_Credential
import project.Errors.S3_Error
import project.Errors.S3_Key_Not_Found
import project.Internal.S3_File_Write_Strategy
import project.Internal.S3_Path.S3_Path
import project.S3.S3

## Represents an S3 file or folder
   If the path ends with a slash, it is a folder. Otherwise, it is a file.
type S3_File
    ## ICON data_input
       Given an S3 URI create a file representation.

       Arguments:
       - uri: The URI of the file.
         The URI must be in the form `s3://bucket/path/to/file`.
         If the path contains `.` or `..` segments, they will be normalized.
       - credentials: The credentials to use when accessing the file.
         If not specified, the default credentials are used.
    new : Text -> AWS_Credential -> S3_File ! Illegal_Argument
    new (uri : Text = S3.uri_prefix) credentials:AWS_Credential=AWS_Credential.Default =
        S3_File.Value (S3_Path.parse uri) credentials

    ## PRIVATE
    Value (s3_path : S3_Path) credentials:AWS_Credential

    ## GROUP Standard.Base.Metadata
       ICON metadata
       Gets the URI of this file
    uri : Text
    uri self -> Text = self.s3_path.to_text

    ## GROUP Standard.Base.Metadata
       ICON metadata
       Checks if the folder or file exists
    exists : Boolean
    exists self = if self.s3_path.bucket == "" then True else
        if self.s3_path.is_root then translate_file_errors self <| S3.head self.s3_path.bucket "" self.credentials . is_error . not else
            pair = translate_file_errors self <| S3.read_bucket self.s3_path.bucket self.s3_path.key self.credentials delimiter=S3_Path.delimiter max_count=1
            pair.second.contains self.s3_path.key

    ## GROUP Standard.Base.Metadata
       ICON metadata
       Checks if this is a folder.
    is_directory : Boolean
    is_directory self = self.s3_path.is_directory

    ## GROUP Standard.Base.Metadata
       Checks if this is a regular file.
    is_regular_file : Boolean
    is_regular_file self = self.is_directory.not

    ## GROUP Standard.Base.Metadata
       ICON metadata
       Gets the size of a file in bytes.
    size : Integer
    size self =
        if self.is_directory then Error.throw (S3_Error.Error "size can only be called on files." self.uri) else
            content_length = translate_file_errors self <| S3.raw_head self.s3_path.bucket self.s3_path.key self.credentials . contentLength
            if content_length.is_nothing then Error.throw (S3_Error.Error "ContentLength header is missing." self.uri) else content_length

    ## PRIVATE
       ADVANCED
       Creates a new output stream for this file and runs the specified action
       on it.

       The created stream is automatically closed when `action` returns (even
       if it returns exceptionally).

       Arguments:
       - open_options: A vector of `File_Access` objects determining how to open
         the stream. These options set the access properties of the stream.
       - action: A function that operates on the output stream and returns some
         value. The value is returned from this method.
    with_output_stream : Vector File_Access -> (Output_Stream -> Any ! File_Error) -> Any ! File_Error
    with_output_stream self (open_options : Vector) action = if self.is_directory then Error.throw (S3_Error.Error "S3 directory cannot be opened as a stream." self.uri) else
        Context.Output.if_enabled disabled_message="Writing to an S3_File is forbidden as the Output context is disabled." panic=False <|
            open_as_data_link = (open_options.contains Data_Link_Access.No_Follow . not) && (Data_Link.is_data_link self)
            if open_as_data_link then Data_Link.write_datalink_as_stream self open_options action else
                if open_options.contains File_Access.Append then Error.throw (S3_Error.Error "S3 does not support appending to a file. Instead you may read it, modify and then write the new contents." self.uri) else
                    # The exists check is not atomic, but it is the best we can do with S3
                    check_exists = open_options.contains File_Access.Create_New
                    valid_options = [File_Access.Write, File_Access.Create_New, File_Access.Truncate_Existing, File_Access.Create]
                    invalid_options = open_options.filter (Filter_Condition.Is_In valid_options action=Filter_Action.Remove)
                    if invalid_options.not_empty then Error.throw (S3_Error.Error "Unsupported S3 stream options: "+invalid_options.to_display_text self.uri) else
                        if check_exists && self.exists then Error.throw (File_Error.Already_Exists self) else
                            # Given that the amount of data written may be large and AWS library does not seem to support streaming it directly, we use a temporary file to store the data.
                            tmp_file = File.create_temporary_file "s3-tmp"
                            Panic.with_finalizer tmp_file.delete <|
                                result = tmp_file.with_output_stream [File_Access.Write] action
                                # Only proceed if the write succeeded
                                result.if_not_error <|
                                    (translate_file_errors self <| S3.upload_file tmp_file self.s3_path.bucket self.s3_path.key self.credentials) . if_not_error <|
                                        result


    ## PRIVATE
       ADVANCED
       Creates a new input stream for this file and runs the specified action
       on it.

       Arguments:
       - open_options: A vector of `File_Access` objects determining how to open
         the stream. These options set the access properties of the stream.
       - action: A function that operates on the input stream and returns some
         value. The value is returned from this method.

       The created stream is automatically closed when `action` returns (even
       if it returns exceptionally).
    with_input_stream : Vector File_Access -> (Input_Stream -> Any ! File_Error) -> Any ! S3_Error | Illegal_Argument
    with_input_stream self (open_options : Vector) action = if self.is_directory then Error.throw (Illegal_Argument.Error "S3 folders cannot be opened as a stream." self.uri) else
        open_as_data_link = (open_options.contains Data_Link_Access.No_Follow . not) && (Data_Link.is_data_link self)
        if open_as_data_link then Data_Link.read_datalink_as_stream self open_options action else
            allowed_options = [File_Access.Read, Data_Link_Access.No_Follow]
            disallowed_options = open_options.filter o-> allowed_options.contains o . not
            if disallowed_options.not_empty then Error.throw (S3_Error.Error "Invalid open options for reading: "+disallowed_options.to_text+".") else
                response_body = translate_file_errors self <| S3.get_object self.s3_path.bucket self.s3_path.key self.credentials delimiter=S3_Path.delimiter
                response_body.with_stream action

    ## ALIAS load, open
       GROUP Standard.Base.Input
       ICON data_input
       Read a file using the specified file format

       Arguments:
       - format: A `File_Format` object used to read file into memory.
         If `Auto_Detect` is specified; the provided file determines the specific
         type and configures it appropriately. If there is no matching type then
         a `File_Error.Unsupported_Type` error is returned.
       - on_problems: Specifies the behavior when a problem occurs during the
         function.
         By default, a warning is issued, but the operation proceeds.
         If set to `Report_Error`, the operation fails with a dataflow error.
         If set to `Ignore`, the operation proceeds without errors or warnings.
    @format File_Format.default_widget
    read : File_Format -> Problem_Behavior -> Any ! S3_Error
    read self format=Auto_Detect (on_problems=Problem_Behavior.Report_Warning) =
        if Data_Link.is_data_link self then Data_Link.read_datalink self format on_problems else
            File_Format.handle_format_missing_arguments format <| case format of
                Auto_Detect -> if self.is_directory then format.read self on_problems else
                    response = translate_file_errors self <| S3.get_object self.s3_path.bucket self.s3_path.key self.credentials delimiter=S3_Path.delimiter
                    response.decode Auto_Detect
                _ ->
                    metadata = File_Format_Metadata.Value path=self.path name=self.name
                    self.with_input_stream [File_Access.Read] (stream-> format.read_stream stream metadata)

    ## ALIAS load bytes, open bytes
       ICON data_input
       Reads all bytes in this file into a byte vector.
    read_bytes : Vector ! File_Error
    read_bytes self =
        self.read Bytes

    ## ALIAS load text, open text
       ICON data_input
       Reads the whole file into a `Text`, with specified encoding.

       Arguments:
       - encoding: The text encoding to decode the file with. Defaults to UTF-8.
       - on_problems: Specifies the behavior when a problem occurs during the
         function.
         By default, a warning is issued, but the operation proceeds.
         If set to `Report_Error`, the operation fails with a dataflow error.
         If set to `Ignore`, the operation proceeds without errors or warnings.
    @encoding Encoding.default_widget
    read_text : Encoding -> Problem_Behavior -> Text ! File_Error
    read_text self (encoding=Encoding.utf_8) (on_problems=Problem_Behavior.Report_Warning) =
        self.read (Plain_Text encoding) on_problems

    ## Deletes the object.
    delete : Nothing
    delete self = if self.is_directory then Error.throw (S3_Error.Error "Deleting S3 folders is currently not implemented." self.uri) else
        if self.exists.not then Error.throw (File_Error.Not_Found self) else
            self.delete_if_exists

    ## Deletes the file if it had existed.
    delete_if_exists : Nothing
    delete_if_exists self = if self.is_directory then Error.throw (S3_Error.Error "Deleting S3 folders is currently not implemented." self.uri) else
        Context.Output.if_enabled disabled_message="Deleting an S3_File is forbidden as the Output context is disabled." panic=False <|
            translate_file_errors self <| S3.delete_object self.s3_path.bucket self.s3_path.key self.credentials . if_not_error Nothing

    ## Copies the file to the specified destination.

       Arguments:
       - destination: the destination to move the file to.
       - replace_existing: specifies if the operation should proceed if the
         destination file already exists. Defaults to `False`.
    copy_to : Writable_File -> Boolean -> Any ! File_Error
    copy_to self (destination : Writable_File) (replace_existing : Boolean = False) =
        if self.is_directory then Error.throw (S3_Error.Error "Copying S3 folders is currently not implemented." self.uri) else
            Context.Output.if_enabled disabled_message="Copying an S3_File is forbidden as the Output context is disabled." panic=False <|
                case destination.file of
                    # Special shortcut for more efficient handling of S3 file copying (no need to move the data to our machine)
                    s3_destination : S3_File ->
                        if replace_existing.not && s3_destination.exists then Error.throw (File_Error.Already_Exists destination) else
                            destination_path = s3_destination.s3_path
                            translate_file_errors self <| S3.copy_object self.s3_path.bucket self.s3_path.key destination_path.bucket destination_path.key self.credentials . if_not_error <| s3_destination
                    _ -> generic_copy self destination.file replace_existing

    ## Moves the file to the specified destination.

       ! S3 Move is a Copy and Delete

         Since S3 does not support moving files, this operation is implemented
         as a copy followed by delete. Keep in mind that the space usage of the
         file will briefly be doubled and that the operation may not be as fast
         as a local move often is.

       Arguments:
       - destination: the destination to move the file to.
       - replace_existing: specifies if the operation should proceed if the
         destination file already exists. Defaults to `False`.
    move_to : Writable_File -> Boolean -> Nothing ! File_Error
    move_to self (destination : Writable_File) (replace_existing : Boolean = False) =
        Context.Output.if_enabled disabled_message="File moving is forbidden as the Output context is disabled." panic=False <|
            r = self.copy_to destination replace_existing=replace_existing
            r.if_not_error <|
                self.delete.if_not_error r

    ## GROUP Standard.Base.Operators
       ICON folder
       Join two path segments together, normalizing the `..` and `.` subpaths.

       Arguments:
       - subpath: The path to join to the path of `self`.

       ! S3 Directory Handling

         Note that regular S3 buckets do not have a 'native' notion of
         directories, instead they are emulated using prefixes and a delimiter
         (in Enso, the delimiter is set to "/").

         The trailing slash determines if the given path is treated as a
         directory or as a regular file.

         However, for ease-of-use, if a path without a trailing slash is used
         with the `/` operator it will be accepted and the sub paths will be
         resolved, even though such a path would not be treated as a directory
         by any other operations.

         See: https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html
    / : Text -> S3_File
    / self subpath =
        S3_File.Value (self.s3_path.resolve subpath) self.credentials

    ## GROUP Standard.Base.Calculations
       ICON folder
       Join two or more path segments together, normalizing the `..` and `.` subpaths.

       Arguments:
       - subpaths: The path segment or segments to join to the path of `self`.

       See `/` for more information about S3 directory handling.
    join : (Vector | Text) -> S3_File
    join self (subpaths : Vector | Text) =
        vec = Vector.unify_vector_or_element subpaths
        vec_as_texts = vec.map subpath-> (subpath : Text)
        S3_File.Value (self.s3_path.join vec_as_texts) self.credentials

    ## GROUP Standard.Base.Metadata
       Resolves the parent of this file.
    parent : S3_File | Nothing
    parent self =
        parent_path = self.s3_path.parent
        parent_path.if_not_nothing <|
            S3_File.Value parent_path self.credentials

    ## GROUP Standard.Base.Metadata
       Checks if `self` is a descendant of `other`.
    is_descendant_of : S3_File -> Boolean
    is_descendant_of self other = self.s3_path.is_descendant_of other.s3_path

    ## GROUP Standard.Base.Metadata
       Returns the path of this file.
    path : Text
    path self = self.s3_path.to_text

    ## GROUP Standard.Base.Metadata
       ICON metadata
       Returns the name of this file.
    name : Text
    name self = self.s3_path.file_name

    ## GROUP Standard.Base.Metadata
       ICON metadata
       Returns the extension of the file.
    extension : Text
    extension self = if self.is_directory then Error.throw (S3_Error.Error "Directories do not have extensions." self.uri) else
        find_extension_from_name self.name

    ## GROUP Standard.Base.Metadata
       Gets the creation time of a file.
    creation_time : Date_Time ! File_Error
    creation_time self =
        Error.throw (S3_Error.Error "Creation time is not available for S3 files, consider using `last_modified_time` instead." self.uri)

    ## GROUP Standard.Base.Metadata
       Gets the last modified time of a file.
    last_modified_time : Date_Time ! File_Error
    last_modified_time self =
        if self.is_directory then Error.throw (S3_Error.Error "`last_modified_time` can only be called on files." self.uri) else
            instant = translate_file_errors self <| S3.raw_head self.s3_path.bucket self.s3_path.key self.credentials . lastModified
            if instant.is_nothing then Error.throw (S3_Error.Error "Missing information for: lastModified" self.uri) else
                instant.at_zone Time_Zone.system

    ## GROUP Standard.Base.Input
       ICON data_input
       Lists files contained in the directory denoted by this file.

       Arguments:
       - name_filter: A glob pattern that can be used to filter the returned
         files. If it is not specified, all files are returned.
       - recursive: Specifies whether the returned list of files should include
         also files from the subdirectories. If set to `False` (the default),
         only the immediate children of the listed directory are considered.

       The `name_filter` can contain the following special characters:

       If `recursive` is set to True and a `name_filter` does not contain `**`,
       it will be automatically prefixed with `**/` to allow matching files in
       subdirectories.
    list : Text -> Boolean -> Vector S3_File
    list self name_filter:Text="" recursive:Boolean=False =
        check_name_filter action = if name_filter != "" then Unimplemented.throw "S3 listing with name filter is not currently implemented." else action
        check_recursion action = if recursive then Unimplemented.throw "S3 listing with recursion is not currently implemented." else action
        check_directory action = if self.is_directory.not then Error.throw (S3_Error.Error "Only folders can be listed." self.uri) else action

        check_directory <| check_recursion <| check_name_filter <|
            if self.s3_path.bucket == "" then translate_file_errors self <| S3.list_buckets self.credentials . map bucket-> S3_File.Value bucket "" self.credentials else
                pair = translate_file_errors self <| S3.read_bucket self.s3_path.bucket self.s3_path.key self.credentials delimiter=S3_Path.delimiter
                bucket = self.s3_path.bucket
                sub_folders = pair.first . map key->
                    S3_File.Value (S3_Path.Value bucket key) self.credentials
                files = pair.second . map key->
                    S3_File.Value (S3_Path.Value bucket key) self.credentials
                sub_folders + files

## PRIVATE
File_Format_Metadata.from (that : S3_File) = File_Format_Metadata.Value that.uri that.name (that.extension.catch _->Nothing)

## PRIVATE
File_Like.from (that : S3_File) = File_Like.Value that

## PRIVATE
Writable_File.from (that : S3_File) =
    Writable_File.Value that S3_File_Write_Strategy.instance

## PRIVATE
   A helper that translates lower level S3 errors to file-system errors.
translate_file_errors related_file result =
    result.catch S3_Key_Not_Found error->
        s3_path = S3_Path.Value error.bucket error.key
        s3_file = S3_File.Value s3_path related_file.credentials
        Error.throw (File_Error.Not_Found s3_file)
